{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efd3bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv(\"bbca_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5b28e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data.drop(index=[0, 1])\n",
    "# data.rename(columns={'Price': 'Date'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59d034a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.to_csv(\"bbca.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c35eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 1: setup =====\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# file paths (ubah kalau beda)\n",
    "PRICE_PATH = \"bbca.csv\"  # <â€” file harga baru kamu\n",
    "FUND_PATH  = \"bbca_fundamentals_quarterly_2021_2023.csv\"\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2034b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 2: load & clean price =====\n",
    "def load_price(path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path)\n",
    "    # drop kolom index kalau ada\n",
    "    for junk in [\"Unnamed: 0\", \"Unnamed: 1\"]:\n",
    "        if junk in df.columns:\n",
    "            df = df.drop(columns=[junk])\n",
    "\n",
    "    assert \"Date\" in df.columns, \"Kolom 'Date' wajib ada.\"\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\")\n",
    "\n",
    "    # pastikan numeric\n",
    "    for c in [\"Close\",\"High\",\"Low\",\"Open\",\"Volume\"]:\n",
    "        if c in df.columns and df[c].dtype == object:\n",
    "            df[c] = df[c].astype(str).str.replace(\",\", \"\", regex=False)\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "    df = df.dropna(subset=[\"Date\"]).sort_values(\"Date\")\n",
    "    # deduplicate per tanggal (ambil baris terakhir)\n",
    "    df = df.groupby(\"Date\", as_index=False).last()\n",
    "    return df\n",
    "\n",
    "price = load_price(PRICE_PATH)\n",
    "price.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ad0fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 3: technical indicators =====\n",
    "def add_technicals(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "\n",
    "    # SMA/EMA\n",
    "    out[\"SMA_20\"] = out[\"Close\"].rolling(window=20, min_periods=20).mean()\n",
    "    out[\"EMA_20\"] = out[\"Close\"].ewm(span=20, adjust=False).mean()\n",
    "\n",
    "    # Bollinger (20, 2)\n",
    "    bb_w, bb_std = 20, 2.0\n",
    "    roll_mean = out[\"Close\"].rolling(bb_w, min_periods=bb_w).mean()\n",
    "    roll_std  = out[\"Close\"].rolling(bb_w, min_periods=bb_w).std()\n",
    "    out[\"Bollinger_upper\"] = roll_mean + bb_std * roll_std\n",
    "    out[\"Bollinger_lower\"] = roll_mean - bb_std * roll_std\n",
    "    out[\"BB_percentB\"]  = (out[\"Close\"] - out[\"Bollinger_lower\"]) / (out[\"Bollinger_upper\"] - out[\"Bollinger_lower\"])\n",
    "    out[\"BB_bandwidth\"] = (out[\"Bollinger_upper\"] - out[\"Bollinger_lower\"]) / (roll_mean + 1e-12)\n",
    "\n",
    "    # RSI(14)\n",
    "    def rsi(series, window=14):\n",
    "        delta = series.diff()\n",
    "        up = delta.clip(lower=0)\n",
    "        down = -delta.clip(upper=0)\n",
    "        roll_up = up.ewm(alpha=1/window, adjust=False).mean()\n",
    "        roll_down = down.ewm(alpha=1/window, adjust=False).mean()\n",
    "        rs = roll_up / (roll_down + 1e-12)\n",
    "        return 100 - (100 / (1 + rs))\n",
    "    out[\"RSI_14\"] = rsi(out[\"Close\"], window=14)\n",
    "\n",
    "    # MACD (12, 26, 9)\n",
    "    fast, slow, signal = 12, 26, 9\n",
    "    ema_fast = out[\"Close\"].ewm(span=fast, adjust=False).mean()\n",
    "    ema_slow = out[\"Close\"].ewm(span=slow, adjust=False).mean()\n",
    "    out[\"MACD_line\"]   = ema_fast - ema_slow\n",
    "    out[\"MACD_signal\"] = out[\"MACD_line\"].ewm(span=signal, adjust=False).mean()\n",
    "    out[\"MACD_hist\"]   = out[\"MACD_line\"] - out[\"MACD_signal\"]\n",
    "\n",
    "    # log-return & volatilitas cepat\n",
    "    out[\"ret_log\"] = np.log(out[\"Close\"] / out[\"Close\"].shift(1))\n",
    "    out[\"roll_std_5\"]  = out[\"ret_log\"].rolling(5,  min_periods=5).std()\n",
    "    out[\"roll_std_10\"] = out[\"ret_log\"].rolling(10, min_periods=10).std()\n",
    "\n",
    "    # kalender (cyclical)\n",
    "    out[\"dayofweek\"] = out[\"Date\"].dt.dayofweek\n",
    "    out[\"month\"]     = out[\"Date\"].dt.month\n",
    "    out[\"day_sin\"]   = np.sin(2*np.pi*out[\"dayofweek\"]/7)\n",
    "    out[\"day_cos\"]   = np.cos(2*np.pi*out[\"dayofweek\"]/7)\n",
    "    out[\"mon_sin\"]   = np.sin(2*np.pi*(out[\"month\"]-1)/12)\n",
    "    out[\"mon_cos\"]   = np.cos(2*np.pi*(out[\"month\"]-1)/12)\n",
    "\n",
    "    return out\n",
    "\n",
    "price = add_technicals(price)\n",
    "price.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc682038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 3b (ADD): lag & momentum features =====\n",
    "def add_lags_and_momentum(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "\n",
    "    # set lag yang umum untuk daily\n",
    "    lag_list = [1, 2, 3, 5, 10, 20]\n",
    "\n",
    "    # kolom yang dilag\n",
    "    base_cols = [\n",
    "        \"ret_log\", \"Close\", \"Volume\",\n",
    "        \"RSI_14\", \"MACD_line\", \"MACD_signal\",\n",
    "        \"BB_percentB\", \"BB_bandwidth\",\n",
    "        \"roll_std_5\", \"roll_std_10\",\n",
    "        \"SMA_20\", \"EMA_20\"\n",
    "    ]\n",
    "    for c in base_cols:\n",
    "        if c in out.columns:\n",
    "            for L in lag_list:\n",
    "                out[f\"{c}_lag{L}\"] = out[c].shift(L)\n",
    "\n",
    "    # momentum / smoothing return\n",
    "    for w in [5, 10, 20]:\n",
    "        if \"ret_log\" in out.columns:\n",
    "            out[f\"ret_log_sma_{w}\"] = out[\"ret_log\"].rolling(w, min_periods=w).mean()\n",
    "            out[f\"ret_log_ema_{w}\"] = out[\"ret_log\"].ewm(span=w, adjust=False).mean()\n",
    "            out[f\"ret_log_cum_{w}\"] = out[\"ret_log\"].rolling(w, min_periods=w).sum()\n",
    "\n",
    "    return out\n",
    "\n",
    "price = add_lags_and_momentum(price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ce8cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 4: fundamentals QoQ/YoY to daily =====\n",
    "def quarter_end_date(q_label: str) -> str:\n",
    "    q_label = (q_label or \"\").upper().strip()\n",
    "    mapping = {\"Q1\":\"03-31\",\"Q2\":\"06-30\",\"Q3\":\"09-30\",\"Q4\":\"12-31\"}\n",
    "    return mapping.get(q_label, \"12-31\")\n",
    "\n",
    "def load_fundamentals(path: str) -> pd.DataFrame:\n",
    "    f = pd.read_csv(path)\n",
    "    f.columns = [c.strip() for c in f.columns]\n",
    "    assert \"Periode\" in f.columns and \"Quartal\" in f.columns, \"Kolom 'Periode' & 'Quartal' wajib ada.\"\n",
    "\n",
    "    years = f[\"Periode\"].astype(str).str.extract(r\"(\\d{4})\")[0]\n",
    "    qend  = f[\"Quartal\"].astype(str).map(quarter_end_date)\n",
    "    f[\"Date\"] = pd.to_datetime(years + \"-\" + qend, errors=\"coerce\")\n",
    "\n",
    "    # bersihkan persen & koma -> numeric\n",
    "    for c in f.columns:\n",
    "        if c not in [\"Date\",\"Periode\",\"Quartal\"]:\n",
    "            if f[c].dtype == object:\n",
    "                f[c] = (f[c].astype(str)\n",
    "                              .str.replace(\"%\",\"\", regex=False)\n",
    "                              .str.replace(\",\",\"\", regex=False))\n",
    "            f[c] = pd.to_numeric(f[c], errors=\"coerce\")\n",
    "\n",
    "    f = f.dropna(subset=[\"Date\"]).sort_values(\"Date\")\n",
    "\n",
    "    num_cols = [c for c in f.columns if c!=\"Date\" and f[c].dtype.kind in \"fcbiu\"]\n",
    "    fq = f[[\"Date\"] + num_cols].copy()\n",
    "\n",
    "    # Î”QoQ & Î”YoY dihitung di level KUARTAL\n",
    "    for col in num_cols:\n",
    "        fq[f\"{col}_QoQ\"] = (fq[col] - fq[col].shift(1)) / fq[col].shift(1).abs()\n",
    "        fq[f\"{col}_YoY\"] = (fq[col] - fq[col].shift(4)) / fq[col].shift(4).abs()\n",
    "\n",
    "    return fq\n",
    "\n",
    "def fundamentals_to_daily(fq: pd.DataFrame, price_dates: pd.Series) -> pd.DataFrame:\n",
    "    daily = fq.set_index(\"Date\").sort_index().ffill().bfill()\n",
    "    daily = daily.reindex(price_dates).ffill().bfill().reset_index().rename(columns={\"index\":\"Date\"})\n",
    "    return daily\n",
    "\n",
    "fund_q = load_fundamentals(FUND_PATH)\n",
    "fund_daily = fundamentals_to_daily(fund_q, price[\"Date\"])\n",
    "\n",
    "# merge semua\n",
    "data = price.merge(fund_daily, on=\"Date\", how=\"left\")\n",
    "print(\"Rows before target:\", len(data))\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5db8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CEK JUMLAH DATA RAW & MERGE (SESUAI NAMA VARIABEL NOTEBOOK) ---\n",
    "\n",
    "def _rng(df, date_col=\"Date\"):\n",
    "    try:\n",
    "        dmin, dmax = df[date_col].min(), df[date_col].max()\n",
    "        return f\"{dmin} â†’ {dmax}\"\n",
    "    except Exception:\n",
    "        return \"-\"\n",
    "\n",
    "print(\"=== RAW TEKNIKAL (price / OHLCV) ===\")\n",
    "print(\"rows:\", len(price), \"| cols:\", len(price.columns), \"| range:\", _rng(price))\n",
    "print(\"kolom:\", sorted(price.columns.tolist())[:12], \"...\")\n",
    "\n",
    "print(\"\\n=== RAW FUNDAMENTAL KUARTALAN (fund_q) ===\")\n",
    "print(\"rows:\", len(fund_q), \"| cols:\", len(fund_q.columns))\n",
    "print(\"kolom:\", sorted(fund_q.columns.tolist())[:12], \"...\")\n",
    "\n",
    "print(\"\\n=== FUNDAMENTAL HARIAN (fund_daily) â€” hasil konversi/expand ===\")\n",
    "print(\"rows:\", len(fund_daily), \"| cols:\", len(fund_daily.columns), \"| range:\", _rng(fund_daily))\n",
    "print(\"kolom:\", sorted(fund_daily.columns.tolist())[:12], \"...\")\n",
    "\n",
    "print(\"\\n=== HASIL MERGE (data = price â‹ˆ fund_daily) â€” sebelum target ===\")\n",
    "print(\"Rows before target:\", len(data), \"| cols:\", len(data.columns), \"| range:\", _rng(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4179c5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 5: target =====\n",
    "# Target = Close(t+1) â€” gampang diganti kalau mau pakai return\n",
    "data[\"target\"] = data[\"Close\"].shift(-1)\n",
    "\n",
    "# (alternatif) target return besok:\n",
    "# data[\"target\"] = data[\"ret_log\"].shift(-1)\n",
    "\n",
    "data = data.dropna().reset_index(drop=True)\n",
    "print(\"Rows after dropna:\", len(data))\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7672082b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 6: split & scaling =====\n",
    "def time_split(df: pd.DataFrame, train_ratio=0.70, val_ratio=0.15):\n",
    "    n = len(df)\n",
    "    train_end = int(n*train_ratio)\n",
    "    val_end   = int(n*(train_ratio+val_ratio))\n",
    "    train = df.iloc[:train_end].copy()\n",
    "    val   = df.iloc[train_end:val_end].copy()\n",
    "    test  = df.iloc[val_end:].copy()\n",
    "    return train, val, test\n",
    "\n",
    "train, val, test = time_split(data, train_ratio=0.70, val_ratio=0.15)\n",
    "\n",
    "exclude = {\"Date\",\"target\"}\n",
    "feature_cols = [c for c in data.columns if c not in exclude and data[c].dtype.kind in \"fcbiu\"]\n",
    "len(feature_cols), feature_cols[:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd79013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 7: baseline & models =====\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(train[feature_cols].values)\n",
    "X_val   = scaler.transform(val[feature_cols].values)\n",
    "X_test  = scaler.transform(test[feature_cols].values)\n",
    "\n",
    "y_train = train[\"target\"].values\n",
    "y_val   = val[\"target\"].values\n",
    "y_test  = test[\"target\"].values\n",
    "\n",
    "def evaluate_regression(y_true, y_pred, name=\"\"):\n",
    "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "    mae  = mean_absolute_error(y_true, y_pred)\n",
    "    r2   = r2_score(y_true, y_pred) if len(np.unique(y_true))>1 else np.nan\n",
    "    print(f\"[{name}] RMSE={rmse:.6f}  MAE={mae:.6f}  RÂ²={r2:.6f}\")\n",
    "    return rmse, mae, r2\n",
    "\n",
    "def price_naive_baseline(y_true):\n",
    "    # baseline harga: t+1 = t\n",
    "    y_true_b = y_true[1:]\n",
    "    y_naive  = np.roll(y_true, 1)[1:]\n",
    "    return mean_squared_error(y_true_b, y_naive, squared=False)\n",
    "\n",
    "print(\"=== BASELINE (Price Naive) ===\")\n",
    "print(\"VAL baseline RMSE :\", price_naive_baseline(y_val))\n",
    "print(\"TEST baseline RMSE:\", price_naive_baseline(y_test))\n",
    "\n",
    "ridge = Ridge(alpha=1.0, random_state=0)\n",
    "ridge.fit(X_train, y_train)\n",
    "y_val_ridge  = ridge.predict(X_val)\n",
    "y_test_ridge = ridge.predict(X_test)\n",
    "print(\"\\n=== RIDGE ===\")\n",
    "rmse_val_ridge, _, _ = evaluate_regression(y_val, y_val_ridge, \"Ridge (val)\")\n",
    "rmse_test_ridge, _, _ = evaluate_regression(y_test, y_test_ridge, \"Ridge (test)\")\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=400, max_depth=None, min_samples_leaf=3, random_state=0, n_jobs=-1)\n",
    "rf.fit(X_train, y_train)\n",
    "y_val_rf  = rf.predict(X_val)\n",
    "y_test_rf = rf.predict(X_test)\n",
    "print(\"\\n=== RANDOM FOREST ===\")\n",
    "rmse_val_rf, _, _ = evaluate_regression(y_val, y_val_rf, \"RF (val)\")\n",
    "rmse_test_rf, _, _ = evaluate_regression(y_test, y_test_rf, \"RF (test)\")\n",
    "\n",
    "best_name = \"ridge\" if rmse_val_ridge < rmse_val_rf else \"rf\"\n",
    "y_test_best = y_test_ridge if best_name==\"ridge\" else y_test_rf\n",
    "rmse_best = mean_squared_error(y_test, y_test_best, squared=False)\n",
    "nrmse_std = rmse_best / (np.std(y_test) + 1e-12)\n",
    "\n",
    "print(f\"\\nBest on VAL: {best_name}\")\n",
    "print(f\"TEST RMSE: {rmse_best:.6f}  |  TEST NRMSE(std): {nrmse_std:.3f}\")\n",
    "print(f\"TEST baseline RMSE: {price_naive_baseline(y_test):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58c4b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 8: plot diag =====\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(y_test, y_test_best, s=8, alpha=0.6)\n",
    "mn, mx = np.nanmin([y_test.min(), y_test_best.min()]), np.nanmax([y_test.max(), y_test_best.max()])\n",
    "plt.plot([mn, mx], [mn, mx], linewidth=2)\n",
    "plt.xlabel(\"Actual Close (t+1)\")\n",
    "plt.ylabel(\"Predicted\")\n",
    "plt.title(f\"Pred vs Actual (Test) - {best_name.upper()}\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56000467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell (baru): lag features sebelum df_tft dipakai =====\n",
    "lag_cols = [\n",
    "    \"Close\",\"Volume\",\"SMA_20\",\"EMA_20\",\"BB_percentB\",\"BB_bandwidth\",\n",
    "    \"RSI_14\",\"MACD_line\",\"MACD_signal\",\"MACD_hist\",\"ret_log\",\n",
    "    \"roll_std_5\",\"roll_std_10\"\n",
    "]\n",
    "for c in lag_cols:\n",
    "    if c in price.columns:\n",
    "        price[c+\"_lag1\"] = price[c].shift(1)\n",
    "        price[c+\"_lag5\"] = price[c].shift(5)\n",
    "\n",
    "# setelah ini lanjut merge ke fund_daily seperti biasa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1990726c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== [PATCH] Cell 10: siapkan data untuk TFT (dengan cleanup NaN) =====\n",
    "from copy import deepcopy\n",
    "\n",
    "fund_daily_shifted = fund_daily.copy()\n",
    "shift_cols = [c for c in fund_daily_shifted.columns if c != \"Date\"]\n",
    "fund_daily_shifted[shift_cols] = fund_daily_shifted[shift_cols].shift(7)\n",
    "\n",
    "df_tft = deepcopy(price.merge(fund_daily_shifted, on=\"Date\", how=\"left\"))\n",
    "\n",
    "# fitur kalender (kalau belum ada di df_tft karena kamu hitungnya di 'price')\n",
    "if \"day_sin\" not in df_tft.columns:\n",
    "    df_tft[\"dayofweek\"] = df_tft[\"Date\"].dt.dayofweek\n",
    "    df_tft[\"month\"]     = df_tft[\"Date\"].dt.month\n",
    "    df_tft[\"day_sin\"]   = np.sin(2*np.pi*df_tft[\"dayofweek\"]/7)\n",
    "    df_tft[\"day_cos\"]   = np.cos(2*np.pi*df_tft[\"dayofweek\"]/7)\n",
    "    df_tft[\"mon_sin\"]   = np.sin(2*np.pi*(df_tft[\"month\"]-1)/12)\n",
    "    df_tft[\"mon_cos\"]   = np.cos(2*np.pi*(df_tft[\"month\"]-1)/12)\n",
    "\n",
    "# daftar known/unknown reals â€” SAMA seperti yang kamu pakai\n",
    "base_known = [\"day_sin\",\"day_cos\",\"mon_sin\",\"mon_cos\"]\n",
    "\n",
    "fund_level_candidates = [\"NPL_Gross (%)\",\"CAR/KPMM (%)\",\"CET-1(%)\",\n",
    "                         \"NII_Midrupiah\",\"Fee_based_Income\",\"CKPN_Midrupiah\"]\n",
    "fund_level = [c for c in fund_level_candidates if c in df_tft.columns]\n",
    "fund_delta = [c for c in df_tft.columns if c.endswith(\"_QoQ\") or c.endswith(\"_YoY\")]\n",
    "\n",
    "known_reals   = [c for c in (base_known + fund_level + fund_delta) if c in df_tft.columns]\n",
    "unknown_reals = [c for c in [\n",
    "    \"Close\",\"Volume\",\"SMA_20\",\"EMA_20\",\n",
    "    \"Bollinger_upper\",\"Bollinger_lower\",\"BB_percentB\",\"BB_bandwidth\",\n",
    "    \"RSI_14\",\"MACD_line\",\"MACD_signal\",\"MACD_hist\",\n",
    "    \"ret_log\",\"roll_std_5\",\"roll_std_10\"\n",
    "] if c in df_tft.columns]\n",
    "\n",
    "# === CLEANUP: drop baris yang masih punya NaN di fitur real + target Close\n",
    "cols_required = list(dict.fromkeys(known_reals + unknown_reals + [\"Close\"]))  # unique, preserve order\n",
    "df_tft = df_tft.dropna(subset=[c for c in cols_required if c in df_tft.columns]).reset_index(drop=True)\n",
    "\n",
    "# tambahkan kolom wajib TFT\n",
    "df_tft[\"group_id\"] = \"BBCA\"\n",
    "df_tft[\"time_idx\"] = np.arange(len(df_tft))\n",
    "\n",
    "# split waktu untuk TFT (karena kita drop warm-up rows, index lama nggak 1:1)\n",
    "def time_split_df(df, train_ratio=0.70, val_ratio=0.15):\n",
    "    n = len(df)\n",
    "    train_end = int(n*train_ratio)\n",
    "    val_end   = int(n*(train_ratio+val_ratio))\n",
    "    return df.iloc[:train_end].copy(), df.iloc[train_end:val_end].copy(), df.iloc[val_end:].copy()\n",
    "\n",
    "train_df, val_df, test_df = time_split_df(df_tft)\n",
    "print(train_df.shape, val_df.shape, test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d84a810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 11 (REPLACE): TimeSeriesDataSet, target=ret_log, H=1 =====\n",
    "from pytorch_forecasting.data import TimeSeriesDataSet\n",
    "from pytorch_forecasting.data.encoders import GroupNormalizer\n",
    "import numpy as np\n",
    "\n",
    "# --- horizon & window ---\n",
    "MAX_ENCODER_LENGTH = 90      # memori encoder (naikkan kalau kuat)\n",
    "MAX_PRED_LENGTH    = 1       # single-step dulu\n",
    "\n",
    "# --- known/unknown reals dasar ---\n",
    "base_known = [\"time_idx\",\"day_sin\",\"day_cos\",\"mon_sin\",\"mon_cos\"]\n",
    "\n",
    "fund_level_candidates = [\"NPL_Gross (%)\",\"CAR/KPMM (%)\",\"CET-1(%)\",\n",
    "                         \"NII_Midrupiah\",\"Fee_based_Income\",\"CKPN_Midrupiah\"]\n",
    "fund_level = [c for c in fund_level_candidates if c in df_tft.columns]\n",
    "fund_delta = [c for c in df_tft.columns if c.endswith(\"_QoQ\") or c.endswith(\"_YoY\")]\n",
    "\n",
    "known_reals = [c for c in (base_known + fund_level + fund_delta) if c in df_tft.columns]\n",
    "unknown_reals = [c for c in [\n",
    "    \"Close\",\"Volume\",\"SMA_20\",\"EMA_20\",\n",
    "    \"Bollinger_upper\",\"Bollinger_lower\",\"BB_percentB\",\"BB_bandwidth\",\n",
    "    \"RSI_14\",\"MACD_line\",\"MACD_signal\",\"MACD_hist\",\n",
    "    \"ret_log\",\"roll_std_5\",\"roll_std_10\"\n",
    "] if c in df_tft.columns]\n",
    "\n",
    "# --- tambahkan semua kolom lag/momentum yang tersedia ---\n",
    "lag_like = [c for c in df_tft.columns\n",
    "            if (\"_lag\" in c) or (\"ret_log_sma_\" in c) or (\"ret_log_ema_\" in c) or (\"ret_log_cum_\" in c)]\n",
    "unknown_reals = sorted(list(set(unknown_reals + lag_like)))\n",
    "\n",
    "# --- SHRINK: batasi feature set biar gak OOM ---\n",
    "must_keep_base = [\n",
    "    \"Close\",\"Volume\",\"SMA_20\",\"EMA_20\",\n",
    "    \"BB_percentB\",\"BB_bandwidth\",\n",
    "    \"RSI_14\",\"MACD_line\",\"MACD_signal\",\"MACD_hist\",\n",
    "    \"ret_log\",\"roll_std_5\",\"roll_std_10\",\n",
    "]\n",
    "keep_lags = [\n",
    "    \"ret_log_lag1\",\"ret_log_lag5\",\n",
    "    \"Close_lag1\",\"Volume_lag1\",\n",
    "    \"RSI_14_lag1\",\"MACD_line_lag1\",\"BB_percentB_lag1\",\n",
    "]\n",
    "keep_momentum = [\"ret_log_sma_5\",\"ret_log_ema_5\",\"ret_log_cum_5\",\n",
    "                 \"ret_log_ema_10\"]   # <-- tambah 1 ini saja dulu (aman memori)\n",
    "allowed_unknown = set(must_keep_base + keep_lags + keep_momentum)\n",
    "unknown_reals = [c for c in unknown_reals if c in allowed_unknown]\n",
    "\n",
    "# --- CLEAN: inf->NaN, drop rows yang masih NaN di kolom wajib ---\n",
    "df_tft = df_tft.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "required_cols = sorted(set(known_reals + unknown_reals + [\"ret_log\"]))  # target=ret_log\n",
    "required_cols = [c for c in required_cols if c in df_tft.columns]\n",
    "\n",
    "df_tft = df_tft.dropna(subset=required_cols).reset_index(drop=True)\n",
    "\n",
    "# --- time_idx harus konsisten setelah drop ---\n",
    "df_tft[\"time_idx\"] = np.arange(len(df_tft))\n",
    "\n",
    "# --- re-split setelah bersih ---\n",
    "def time_split_df(df, train_ratio=0.70, val_ratio=0.15):\n",
    "    n = len(df); tr = int(n*train_ratio); va = int(n*(train_ratio+val_ratio))\n",
    "    return df.iloc[:tr].copy(), df.iloc[tr:va].copy(), df.iloc[va:].copy()\n",
    "\n",
    "train_df, val_df, test_df = time_split_df(df_tft)\n",
    "print(\"Shapes after clean/shrink:\", train_df.shape, val_df.shape, test_df.shape)\n",
    "print(\"known_reals:\", len(known_reals), \"| unknown_reals:\", len(unknown_reals))\n",
    "\n",
    "# --- bangun dataset TFT ---\n",
    "training = TimeSeriesDataSet(\n",
    "    train_df,\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"ret_log\",                         # target = log-return\n",
    "    group_ids=[\"group_id\"],\n",
    "    max_encoder_length=MAX_ENCODER_LENGTH,\n",
    "    max_prediction_length=MAX_PRED_LENGTH,\n",
    "    static_categoricals=[\"group_id\"],\n",
    "    time_varying_known_reals=sorted(set(known_reals + [\"time_idx\"])),\n",
    "    time_varying_unknown_reals=unknown_reals,\n",
    "    target_normalizer=GroupNormalizer(groups=[\"group_id\"]),\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    ")\n",
    "\n",
    "validation = TimeSeriesDataSet.from_dataset(training, val_df, predict=True, stop_randomization=True)\n",
    "testing    = TimeSeriesDataSet.from_dataset(training, test_df, predict=True, stop_randomization=True)\n",
    "\n",
    "# VAL untuk training/early-stopping: predict=False (sliding windows)\n",
    "val_ds_train = TimeSeriesDataSet.from_dataset(training, val_df, predict=False, stop_randomization=True)\n",
    "val_dataloader_train = val_ds_train.to_dataloader(train=False, batch_size=16, num_workers=0)\n",
    "\n",
    "# --- dataloaders (batch kecil biar aman memori) ---\n",
    "train_dataloader = training.to_dataloader(train=True,  batch_size=16, num_workers=0)\n",
    "val_dataloader   = validation.to_dataloader(train=False, batch_size=16, num_workers=0)\n",
    "test_dataloader  = testing.to_dataloader(train=False, batch_size=16, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fc1197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 12 (FINAL REPLACE): Train TFT + Robust Manual Load (no load_from_checkpoint) =====\n",
    "import torch, inspect, importlib, pandas as pd\n",
    "import pytorch_forecasting as pf\n",
    "from pytorch_forecasting import TemporalFusionTransformer\n",
    "from pytorch_forecasting.metrics import MAE\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "seed_everything(42, workers=True)\n",
    "\n",
    "# --- 1) Definisi model dari dataset (sama persis utk reload) ---\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=1e-3,          # lebih kalem\n",
    "    hidden_size=64,\n",
    "    attention_head_size=2,\n",
    "    dropout=0.1,\n",
    "    loss=MAE(),                  # pakai MAE\n",
    "    log_interval=-1,\n",
    "    reduce_on_plateau_patience=4,\n",
    ")\n",
    "\n",
    "# --- 2) Callbacks & Trainer ---\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=8, mode=\"min\")\n",
    "ckpt = ModelCheckpoint(\n",
    "    dirpath=\"checkpoints\",\n",
    "    filename=\"tft-{epoch:02d}-{val_loss:.4f}\",\n",
    "    monitor=\"val_loss\", save_top_k=1, mode=\"min\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=80,\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    gradient_clip_val=0.1,\n",
    "    callbacks=[early_stop, ckpt],\n",
    "    log_every_n_steps=1,\n",
    "    enable_progress_bar=True,\n",
    "    logger=False,\n",
    ")\n",
    "\n",
    "# --- 3) Train ---\n",
    "trainer.fit(tft, train_dataloader, val_dataloader_train)\n",
    "\n",
    "best_path = ckpt.best_model_path\n",
    "print(\"Best checkpoint:\", best_path)\n",
    "\n",
    "# ===== 4) Robust manual load (hindari UnpicklingError PyTorch 2.6) =====\n",
    "def _resolve(fqn: str):\n",
    "    \"\"\"Import object by fully-qualified name; return None jika tidak ada.\"\"\"\n",
    "    try:\n",
    "        mod, name = fqn.rsplit(\".\", 1)\n",
    "        return getattr(importlib.import_module(mod), name)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# Allowlist encoder pytorch-forecasting + tipe/func pandas yang sering tersimpan di ckpt\n",
    "allowed = []\n",
    "enc_mod = pf.data.encoders\n",
    "for name in [\"GroupNormalizer\",\"EncoderNormalizer\",\"NaNLabelEncoder\",\"MultiNormalizer\",\"TorchNormalizer\"]:\n",
    "    if hasattr(enc_mod, name) and inspect.isclass(getattr(enc_mod, name)):\n",
    "        allowed.append(getattr(enc_mod, name))\n",
    "\n",
    "for fqn in [\n",
    "    \"pandas.core.frame.DataFrame\",\n",
    "    \"pandas.core.series.Series\",\n",
    "    \"pandas.core.indexes.base.Index\",\n",
    "    \"pandas.core.indexes.range.RangeIndex\",\n",
    "    \"pandas.core.indexes.datetimes.DatetimeIndex\",\n",
    "    \"pandas.core.indexes.multi.MultiIndex\",\n",
    "    \"pandas.core.internals.managers.BlockManager\",\n",
    "    \"pandas.core.internals.blocks.Block\",\n",
    "    \"pandas.core.arrays.categorical.Categorical\",\n",
    "    \"pandas._libs.internals._unpickle_block\",   # beberapa versi pandas butuh ini\n",
    "]:\n",
    "    obj = _resolve(fqn)\n",
    "    if obj is not None:\n",
    "        allowed.append(obj)\n",
    "\n",
    "# --- 5) Muat ckpt sebagai dict & inject state_dict ke model baru (AMAN) ---\n",
    "if best_path:\n",
    "    # (a) load raw checkpoint dict (weights_only=False) di dalam safe context\n",
    "    with torch.serialization.safe_globals(allowed):\n",
    "        ckpt_raw = torch.load(best_path, map_location=device, weights_only=False)\n",
    "\n",
    "    # (b) bangun ulang arsitektur yg identik\n",
    "    tft_loaded = TemporalFusionTransformer.from_dataset(\n",
    "        training,\n",
    "        learning_rate=1e-3,\n",
    "        hidden_size=64,\n",
    "        attention_head_size=2,\n",
    "        dropout=0.1,\n",
    "        loss=MAE(),\n",
    "        log_interval=-1,\n",
    "        reduce_on_plateau_patience=4,\n",
    "    )\n",
    "\n",
    "    # (c) ambil state_dict (kompatibel berbagai format ckpt)\n",
    "    sd = ckpt_raw.get(\"state_dict\", ckpt_raw)\n",
    "\n",
    "    # (d) load bobot\n",
    "    missing, unexpected = tft_loaded.load_state_dict(sd, strict=False)\n",
    "    if missing or unexpected:\n",
    "        print(\"[INFO] load_state_dict warnings:\", {\"missing\": missing, \"unexpected\": unexpected})\n",
    "\n",
    "    tft = tft_loaded.to(device).eval()\n",
    "    print(\"âœ… Checkpoint loaded via manual state_dict (no load_from_checkpoint)\")\n",
    "else:\n",
    "    print(\"[WARN] best_model_path kosong; cek callback ModelCheckpoint/monitor\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f22987",
   "metadata": {},
   "outputs": [],
   "source": [
    "tft.load_state_dict(torch.load(\"tft_model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7bd674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 13 (REPLACE): evaluasi untuk target=log-return =====\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Dataset & dataloader KHUSUS evaluasi (sliding windows), tidak dipakai saat training\n",
    "eval_val_ds  = TimeSeriesDataSet.from_dataset(training, val_df,  predict=False, stop_randomization=True)\n",
    "eval_test_ds = TimeSeriesDataSet.from_dataset(training, test_df, predict=False, stop_randomization=True)\n",
    "\n",
    "eval_val_loader  = eval_val_ds.to_dataloader(train=False, batch_size=64, num_workers=0)\n",
    "eval_test_loader = eval_test_ds.to_dataloader(train=False, batch_size=64, num_workers=0)\n",
    "\n",
    "def _to_np(t):\n",
    "    a = t.detach().cpu().numpy()\n",
    "    if a.ndim == 3 and a.shape[-1] == 1:\n",
    "        a = a[..., 0]   # (N,H,1) -> (N,H)\n",
    "    return a\n",
    "\n",
    "def eval_tft_returns(dataloader, name=\"VAL\"):\n",
    "    # pakai mode=\"raw\" supaya kita bisa ambil decoder_target & decoder_lengths\n",
    "    raw, x = tft.predict(dataloader, mode=\"raw\", return_x=True)\n",
    "\n",
    "    y_pred_all = _to_np(raw[\"prediction\"])          # (N,H)\n",
    "    y_true_all = _to_np(x[\"decoder_target\"])        # (N,H)\n",
    "    dec_len    = _to_np(x[\"decoder_lengths\"]).astype(int).reshape(-1)  # (N,)\n",
    "\n",
    "    # pastikan 2D\n",
    "    if y_pred_all.ndim == 1: y_pred_all = y_pred_all[:, None]\n",
    "    if y_true_all.ndim == 1: y_true_all = y_true_all[:, None]\n",
    "\n",
    "    N, H = y_true_all.shape\n",
    "\n",
    "    # mask horizon valid â†’ flatten\n",
    "    mask   = (np.arange(H)[None, :] < dec_len[:, None])\n",
    "    y_true = y_true_all[mask].astype(float)\n",
    "    y_pred = y_pred_all[mask].astype(float)\n",
    "\n",
    "    # baseline untuk RETURN = 0 (arti: ekspektasi return = 0)\n",
    "    y_base = np.zeros_like(y_true)\n",
    "\n",
    "    # filter finite\n",
    "    m = np.isfinite(y_true) & np.isfinite(y_pred)\n",
    "    y_true = y_true[m]; y_pred = y_pred[m]; y_base = y_base[m]\n",
    "\n",
    "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "    mae  = mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "    # RÂ² bisa NaN kalau var(y_true)=0\n",
    "    r2 = r2_score(y_true, y_pred) if len(np.unique(y_true)) > 1 else np.nan\n",
    "\n",
    "    # NRMSE by std â€” kalau std â‰ˆ 0, set NaN biar gak misleading\n",
    "    std = np.std(y_true)\n",
    "    nrmse_std = rmse / std if std > 1e-8 else np.nan\n",
    "\n",
    "    rmse_baseline = mean_squared_error(y_true, y_base, squared=False)\n",
    "\n",
    "    print(f\"\\nðŸ“Š {name} â€” TFT (target=log-return)\")\n",
    "    print(f\"RMSE={rmse:.6f} | MAE={mae:.6f} | RÂ²={r2:.4f} | baseline RMSE={rmse_baseline:.6f} | NRMSE(std)={(nrmse_std if nrmse_std==nrmse_std else float('nan')):.3f}\")\n",
    "\n",
    "    return {\n",
    "        \"y_true_all\": y_true_all,  # <-- dibalikin lagi utk Cell 14\n",
    "        \"y_pred_all\": y_pred_all,\n",
    "        \"dec_len\": dec_len,\n",
    "        \"rmse\": rmse,\n",
    "        \"rmse_baseline\": rmse_baseline,\n",
    "        \"mae\": mae,\n",
    "        \"r2\": r2,\n",
    "        \"nrmse_std\": nrmse_std,\n",
    "    }\n",
    "\n",
    "val_eval  = eval_tft_returns(eval_val_loader,  \"VAL\")\n",
    "test_eval = eval_tft_returns(eval_test_loader, \"TEST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b6706d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 14 (REPLACE): per-horizon metrics langsung dari model =====\n",
    "def per_horizon_from_model(dataloader, title=\"VAL\"):\n",
    "    raw, x = tft.predict(dataloader, mode=\"raw\", return_x=True)\n",
    "\n",
    "    y_pred_all = _to_np(raw[\"prediction\"])\n",
    "    y_true_all = _to_np(x[\"decoder_target\"])\n",
    "    dec_len    = _to_np(x[\"decoder_lengths\"]).astype(int).reshape(-1)\n",
    "\n",
    "    if y_pred_all.ndim == 1: y_pred_all = y_pred_all[:, None]\n",
    "    if y_true_all.ndim == 1: y_true_all = y_true_all[:, None]\n",
    "\n",
    "    H = y_true_all.shape[1]\n",
    "    print(f\"\\nðŸ”Ž Per-horizon â€” {title} (H={H})\")\n",
    "\n",
    "    for h in range(H):\n",
    "        valid = dec_len > h\n",
    "        if valid.sum() == 0: \n",
    "            continue\n",
    "        yt = y_true_all[valid, h].astype(float)\n",
    "        yp = y_pred_all[valid, h].astype(float)\n",
    "        m  = np.isfinite(yt) & np.isfinite(yp)\n",
    "        yt, yp = yt[m], yp[m]\n",
    "        if len(yt) == 0: \n",
    "            continue\n",
    "\n",
    "        rmse_h = mean_squared_error(yt, yp, squared=False)\n",
    "        r2_h   = r2_score(yt, yp) if len(np.unique(yt))>1 else np.nan\n",
    "        print(f\"H{h+1}: RMSE={rmse_h:.6f} | RÂ²={r2_h:.4f}\")\n",
    "\n",
    "per_horizon_from_model(eval_val_loader,  \"VAL\")\n",
    "per_horizon_from_model(eval_test_loader, \"TEST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb67c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 15: interpretability â€” variable importance =====\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "raw_val, x_val = tft.predict(val_dataloader, mode=\"raw\", return_x=True)\n",
    "interp = tft.interpret_output(raw_val, reduction=\"sum\")  # global\n",
    "\n",
    "def plot_vi(names, scores, title):\n",
    "    order = np.argsort(scores)[::-1]\n",
    "    names  = [names[i] for i in order]\n",
    "    scores = [scores[i] for i in order]\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.barh(range(len(scores)), scores[::-1])\n",
    "    plt.yticks(range(len(scores)), names[::-1])\n",
    "    plt.title(title); plt.xlabel(\"Importance\")\n",
    "    plt.grid(True, axis=\"x\", alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "if \"encoder_variable_importance\" in interp and \"encoder_variables\" in interp:\n",
    "    enc_names  = interp[\"encoder_variables\"]\n",
    "    enc_scores = interp[\"encoder_variable_importance\"].detach().cpu().numpy()\n",
    "    plot_vi(enc_names, enc_scores, \"Encoder variable importance\")\n",
    "\n",
    "if \"decoder_variable_importance\" in interp and \"decoder_variables\" in interp:\n",
    "    dec_names  = interp[\"decoder_variables\"]\n",
    "    dec_scores = interp[\"decoder_variable_importance\"].detach().cpu().numpy()\n",
    "    plot_vi(dec_names, dec_scores, \"Decoder variable importance\")\n",
    "\n",
    "if \"static_variables\" in interp and \"static_variable_importance\" in interp:\n",
    "    st_names  = interp[\"static_variables\"]\n",
    "    st_scores = interp[\"static_variable_importance\"].detach().cpu().numpy()\n",
    "    plot_vi(st_names, st_scores, \"Static variable importance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c461e8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 16 (REPLACE): Interpretabilitas TFT (aman) =====\n",
    "import numpy as np\n",
    "\n",
    "# pakai loader evaluasi yang predict=False biar sampel banyak (sudah dibuat di Cell 13)\n",
    "raw_val, x_val = tft.predict(eval_val_loader, mode=\"raw\", return_x=True)\n",
    "\n",
    "# beberapa versi bisa balik list; normalkan ke dict\n",
    "if isinstance(raw_val, list):\n",
    "    # gabung batch pertama saja untuk interpretasi cepat\n",
    "    raw_val = raw_val[0]\n",
    "\n",
    "# 1) Variable importance dari Variable Selection Network\n",
    "#    reduction harus \"mean\" atau \"sum\" (bukan None)\n",
    "interpv = tft.interpret_output(raw_val, reduction=\"mean\")   # <-- FIX di sini\n",
    "\n",
    "def _agg_importance(a):\n",
    "    a = np.array(a)\n",
    "    # rata-rata absolut di semua axis kecuali axis variabel (terakhir)\n",
    "    if a.ndim == 1:\n",
    "        return np.abs(a)\n",
    "    axes = tuple(range(a.ndim - 1))\n",
    "    return np.nanmean(np.abs(a), axis=axes)\n",
    "\n",
    "enc_imp = _agg_importance(interpv.get(\"encoder_variables\"))\n",
    "dec_imp = _agg_importance(interpv.get(\"decoder_variables\"))\n",
    "\n",
    "# ambil nama variabel: coba dari hparams, fallback ke dataset\n",
    "try:\n",
    "    var_names = list(tft.hparams.x_reals)\n",
    "except Exception:\n",
    "    try:\n",
    "        # beberapa versi PF simpan di dataset\n",
    "        var_names = list(training.reals)\n",
    "    except Exception:\n",
    "        var_names = [f\"var_{i}\" for i in range(len(enc_imp))]\n",
    "\n",
    "def _topk(names, scores, k=15):\n",
    "    idx = np.argsort(scores)[::-1][:min(k, len(scores))]\n",
    "    return [(names[i], float(scores[i])) for i in idx]\n",
    "\n",
    "top_enc = _topk(var_names, enc_imp, k=15)\n",
    "top_dec = _topk(var_names, dec_imp, k=15)\n",
    "\n",
    "print(\"\\nðŸ”Ž Encoder variable importance (top 15):\")\n",
    "for n,s in top_enc:\n",
    "    print(f\"{n:20s}  {s:.6f}\")\n",
    "\n",
    "print(\"\\nðŸ”Ž Decoder variable importance (top 15):\")\n",
    "for n,s in top_dec:\n",
    "    print(f\"{n:20s}  {s:.6f}\")\n",
    "\n",
    "# 2) Attention over encoder timesteps (lag mana yang paling diperhatikan)\n",
    "attn = interpv.get(\"attention\", None)\n",
    "if attn is not None:\n",
    "    A = np.array(attn)\n",
    "    # bentuk umum: (batch, heads, dec_time, enc_time)\n",
    "    # rata-rata semua kecuali dim terakhir (enc_time)\n",
    "    axes = tuple(range(A.ndim - 1))\n",
    "    enc_time_scores = np.nanmean(A, axis=axes) if A.ndim > 1 else A\n",
    "    # ranking lag encoder (0=posisi paling lama di encoder, -1=terdekat)\n",
    "    enc_steps = np.arange(enc_time_scores.shape[-1])\n",
    "    idx = np.argsort(enc_time_scores)[::-1][:10]\n",
    "    print(\"\\nðŸŽ¯ Top attention encoder steps (terbesar -> terkecil):\")\n",
    "    for i in idx:\n",
    "        print(f\"encoder_step={int(enc_steps[i])}, score={float(enc_time_scores[i]):.6f}\")\n",
    "else:\n",
    "    print(\"\\n(i) Attention tidak tersedia pada objek output ini â€” tetap bisa pakai variable importance di atas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa45f25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 17: kumpulkan prediksi & align ke tanggal =====\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# helper: kalau _to_np belum ada (mis. kamu clear kernel), define lagi\n",
    "try:\n",
    "    _to_np\n",
    "except NameError:\n",
    "    def _to_np(t):\n",
    "        a = t.detach().cpu().numpy()\n",
    "        if a.ndim == 3 and a.shape[-1] == 1:\n",
    "            a = a[..., 0]\n",
    "        return a\n",
    "\n",
    "def collect_pred_df(dataloader, name=\"TEST\"):\n",
    "    raw, x, idx = tft.predict(dataloader, mode=\"raw\", return_x=True, return_index=True)\n",
    "\n",
    "    # (N,H) flatten -> 1D\n",
    "    y_pred_all = _to_np(raw[\"prediction\"])\n",
    "    y_true_all = _to_np(x[\"decoder_target\"])\n",
    "    if y_pred_all.ndim == 1: y_pred_all = y_pred_all[:, None]\n",
    "    if y_true_all.ndim == 1: y_true_all = y_true_all[:, None]\n",
    "    N, H = y_true_all.shape\n",
    "\n",
    "    # ambil time_idx horizon-0 (H=1 â†’ satu kolom)\n",
    "    # coba dari x[\"decoder_time_idx\"], fallback ke idx[\"time_idx\"]\n",
    "    if \"decoder_time_idx\" in x:\n",
    "        t_idx = _to_np(x[\"decoder_time_idx\"]).astype(int)\n",
    "        t_idx_h0 = t_idx[:, 0] if t_idx.ndim == 2 else t_idx.reshape(-1)\n",
    "    else:\n",
    "        # idx bisa DataFrame dengan kolom 'time_idx' per sampel\n",
    "        if isinstance(idx, (pd.DataFrame, pd.Series)) and \"time_idx\" in idx:\n",
    "            t_idx_h0 = idx[\"time_idx\"].to_numpy().astype(int).reshape(-1)\n",
    "        else:\n",
    "            # fallback konservatif: numbering urut (kurang ideal)\n",
    "            t_idx_h0 = np.arange(N)\n",
    "\n",
    "                # flatten ke 1D (H=1)\n",
    "    y_pred = y_pred_all[:, 0]\n",
    "    y_true = y_true_all[:, 0]\n",
    "\n",
    "    # map ke Date & Close dari df_tft\n",
    "    map_df = df_tft[[\"time_idx\",\"Date\",\"Close\"]].copy()\n",
    "    map_df[\"Close_prev\"] = map_df[\"Close\"].shift(1)\n",
    "    t2date = dict(zip(map_df[\"time_idx\"], map_df[\"Date\"]))\n",
    "    t2close = dict(zip(map_df[\"time_idx\"], map_df[\"Close\"]))\n",
    "    t2close_prev = dict(zip(map_df[\"time_idx\"], map_df[\"Close_prev\"]))\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"split\": name,\n",
    "        \"time_idx\": t_idx_h0,\n",
    "        \"ret_true\": y_true,\n",
    "        \"ret_pred\": y_pred,\n",
    "    })\n",
    "    df[\"Date\"] = df[\"time_idx\"].map(t2date)\n",
    "    df[\"Close_true\"] = df[\"time_idx\"].map(t2close)            # Close(t)\n",
    "    df[\"Close_prev\"] = df[\"time_idx\"].map(lambda t: t2close_prev.get(t))  # Close(t-1)\n",
    "\n",
    "    # prediksi harga H1 dari return: P_t_hat = P_{t-1} * exp(ret_pred)\n",
    "    df[\"Close_pred\"] = df[\"Close_prev\"] * np.exp(df[\"ret_pred\"])\n",
    "    # baseline harga: hold-last\n",
    "    df[\"Close_base\"] = df[\"Close_prev\"]\n",
    "    df[\"year\"] = df[\"Date\"].dt.year\n",
    "    # buang baris yang masih NaN (awal seri biasanya)\n",
    "    df = df.dropna(subset=[\"Date\",\"Close_true\",\"Close_prev\",\"Close_pred\"]).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "df_val  = collect_pred_df(eval_val_loader,  \"VAL\")\n",
    "df_test = collect_pred_df(eval_test_loader, \"TEST\")\n",
    "\n",
    "print(df_val.head(3))\n",
    "print(df_test.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0692d7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- SAVE TRAINED MODEL ---\n",
    "# print(\"Saving TFT model as tft_model.pth ...\")\n",
    "# import torch\n",
    "# torch.save(tft.state_dict(), \"tft_model.pth\")\n",
    "# print(\"All files saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49442598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SAVE PREDICTION RESULT ---\n",
    "print(\"Saving df_test as predicted_tft.csv ...\")\n",
    "df_test.to_csv(\"predicted_tft.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ee9975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 18: plot garis harga aktual vs prediksi (TEST) =====\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dfp = df_test.sort_values(\"Date\")\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(dfp[\"Date\"], dfp[\"Close_true\"], label=\"Actual Close\")\n",
    "plt.plot(dfp[\"Date\"], dfp[\"Close_pred\"], label=\"Predicted Close (H1)\")\n",
    "plt.title(\"BBCA â€” Actual vs Predicted Close (H1) â€” TEST\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Price\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3992d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 19: scatter return pred vs aktual (TEST) =====\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "dfr = df_test.dropna(subset=[\"ret_true\",\"ret_pred\"])\n",
    "x = dfr[\"ret_true\"].values\n",
    "y = dfr[\"ret_pred\"].values\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(x, y, alpha=0.5, s=16)\n",
    "lims = [min(x.min(), y.min()), max(x.max(), y.max())]\n",
    "plt.plot(lims, lims, linestyle=\"--\")  # y=x\n",
    "plt.title(\"Predicted vs Actual log-return (H1) â€” TEST\")\n",
    "plt.xlabel(\"Actual ret_log\")\n",
    "plt.ylabel(\"Predicted ret_log\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# korelasi sederhana\n",
    "corr = np.corrcoef(x, y)[0,1]\n",
    "print(f\"Correlation (TEST) = {corr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cae77fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 20: residual harga (TEST) =====\n",
    "import matplotlib.pyplot as plt\n",
    "res = (df_test[\"Close_true\"] - df_test[\"Close_pred\"]).values\n",
    "\n",
    "plt.figure(figsize=(12,3))\n",
    "plt.plot(df_test[\"Date\"], res)\n",
    "plt.axhline(0, linestyle=\"--\")\n",
    "plt.title(\"Residuals (Actual - Predicted) Close â€” TEST\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Residual\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(res, bins=40)\n",
    "plt.title(\"Histogram Residuals (Close) â€” TEST\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dc9a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 21: barplot variable importance encoder & decoder =====\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def barplot_top(pairs, title):\n",
    "    if not pairs:\n",
    "        print(f\"(i) No data for {title}\")\n",
    "        return\n",
    "    names = [p[0] for p in pairs][::-1]\n",
    "    vals  = [p[1] for p in pairs][::-1]\n",
    "    plt.figure(figsize=(8, max(3, 0.4*len(names))))\n",
    "    plt.barh(range(len(names)), vals)\n",
    "    plt.yticks(range(len(names)), names)\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "barplot_top(top_enc, \"Encoder Variable Importance (avg |contribution|)\")\n",
    "barplot_top(top_dec, \"Decoder Variable Importance (avg |contribution|)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7360fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 22 (REPLACE): Temporal attention vs Date â€” robust untuk attention agregat =====\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def _safe_to_np(t):\n",
    "    if t is None:\n",
    "        return None\n",
    "    a = t.detach().cpu().numpy()\n",
    "    if a.ndim == 3 and a.shape[-1] == 1:\n",
    "        a = a[..., 0]\n",
    "    return a\n",
    "\n",
    "def _get_attention_from_raw(raw):\n",
    "    \"\"\"Return attention with shape (N, dec_len, enc_len) if possible.\"\"\"\n",
    "    attn = None\n",
    "    if isinstance(raw, dict):\n",
    "        attn = raw.get(\"attention\", None)\n",
    "    if attn is None and isinstance(raw, list) and len(raw) > 0 and isinstance(raw[0], dict):\n",
    "        attn = raw[0].get(\"attention\", None)\n",
    "    if attn is None:\n",
    "        return None\n",
    "\n",
    "    A = np.array(attn)\n",
    "    if A.ndim == 4:        # (N, heads, dec_len, enc_len)\n",
    "        A = A.mean(axis=1) # -> (N, dec_len, enc_len)\n",
    "    elif A.ndim == 3:      # (N, dec_len, enc_len)\n",
    "        pass\n",
    "    elif A.ndim == 2:      # (N, enc_len) atau (dec_len, enc_len) tanpa batch\n",
    "        # asumsikan sudah agregat â†’ tambahkan dim batch & dec_len kalau perlu\n",
    "        if A.shape[0] != A.shape[1]:      # (dec_len, enc_len)\n",
    "            A = A[None, ...]              # -> (1, dec_len, enc_len)\n",
    "        else:                             \n",
    "            A = A[:, None, :]             # -> (N, 1, enc_len)\n",
    "    else:\n",
    "        return None\n",
    "    return A\n",
    "\n",
    "def plot_temporal_attention_by_date(\n",
    "    dataloader,\n",
    "    split_name=\"TEST\",\n",
    "    target_date=None,   # e.g. \"2025-07-31\"\n",
    "    decoder_h=0,\n",
    "    overlay_price=True\n",
    "):\n",
    "    # ambil raw + x + index\n",
    "    raw, x, idx = tft.predict(dataloader, mode=\"raw\", return_x=True, return_index=True)\n",
    "    raw0 = raw[0] if isinstance(raw, list) and len(raw) > 0 else raw\n",
    "\n",
    "    # ambil attention\n",
    "    A = _get_attention_from_raw(raw0)\n",
    "    aggregated = False\n",
    "    if A is None:\n",
    "        # fallback: interpret_output (agregat, tanpa batch per-sampel)\n",
    "        try:\n",
    "            interpv = tft.interpret_output(raw0, reduction=\"mean\")\n",
    "            A = interpv.get(\"attention\", None)\n",
    "            if A is None:\n",
    "                print(\"(i) Attention tidak tersedia pada output ini.\")\n",
    "                return\n",
    "            A = np.array(A)\n",
    "            if A.ndim == 1: A = A[None, :]\n",
    "            if A.ndim == 2: A = A[None, ...]  # -> (1, dec_len, enc_len)\n",
    "            aggregated = True\n",
    "            print(\"(i) Menggunakan attention *rata-rata* (agregat).\")\n",
    "        except Exception as e:\n",
    "            print(\"(i) Attention tidak tersedia dan interpret_output gagal:\", e)\n",
    "            return\n",
    "\n",
    "    # decoder t0 (waktu prediksi)\n",
    "    dec_ti = _safe_to_np(x.get(\"decoder_time_idx\"))\n",
    "    if dec_ti is not None:\n",
    "        dec_ti = dec_ti.astype(int)\n",
    "        if dec_ti.ndim == 1:\n",
    "            dec_ti = dec_ti[:, None]\n",
    "        dec_t0 = dec_ti[:, 0]\n",
    "    else:\n",
    "        if isinstance(idx, (pd.DataFrame, pd.Series)) and \"time_idx\" in idx:\n",
    "            dec_t0 = idx[\"time_idx\"].to_numpy().astype(int).reshape(-1)\n",
    "            dec_ti = dec_t0[:, None]\n",
    "        else:\n",
    "            raise RuntimeError(\"Tidak bisa memperoleh decoder time_idx dari x maupun idx.\")\n",
    "\n",
    "    N_x = len(dec_t0)                 # jumlah sampel di loader\n",
    "    N_A = A.shape[0]                  # jumlah batch di attention\n",
    "\n",
    "    # mapping time_idx -> Date / Close\n",
    "    t2date  = dict(zip(df_tft[\"time_idx\"].to_numpy(), pd.to_datetime(df_tft[\"Date\"])))\n",
    "    t2close = dict(zip(df_tft[\"time_idx\"].to_numpy(), df_tft[\"Close\"].to_numpy()))\n",
    "\n",
    "    # pilih sampel index\n",
    "    if target_date is not None:\n",
    "        target_ts = pd.to_datetime(target_date)\n",
    "        dec_dates = np.array([t2date.get(int(t), pd.NaT) for t in dec_t0], dtype=\"datetime64[ns]\")\n",
    "        valid = ~pd.isna(dec_dates)\n",
    "        if valid.sum() == 0:\n",
    "            print(\"Tidak ada sampel dengan decoder date yang valid.\")\n",
    "            return\n",
    "        deltas = np.abs(dec_dates[valid] - target_ts)\n",
    "        n_idx_x = np.arange(N_x)[valid][np.argmin(deltas)]\n",
    "    else:\n",
    "        target_ts = None\n",
    "        n_idx_x = N_x - 1  # pakai yang terbaru\n",
    "\n",
    "    # pilih horizon\n",
    "    dec_len = A.shape[1]\n",
    "    h = min(decoder_h, dec_len - 1)\n",
    "\n",
    "    # siapkan encoder index vector & attention vector dengan panjang sama\n",
    "    enc_len_att = A.shape[2]\n",
    "\n",
    "    if N_A == N_x:\n",
    "        # attention per-sampel tersedia â†’ pilih baris n_idx_x\n",
    "        att_vec = A[n_idx_x, h, :]  # (enc_len,)\n",
    "        # ambil encoder_time_idx jika ada, kalau tidak rekonstruksi dari encoder_lengths\n",
    "        enc_ti = _safe_to_np(x.get(\"encoder_time_idx\"))\n",
    "        if enc_ti is not None:\n",
    "            if enc_ti.ndim == 1: enc_ti = enc_ti[None, :]\n",
    "            enc_idx_vec = enc_ti[n_idx_x]\n",
    "            # align kanan: ambil tail sepanjang enc_len_att\n",
    "            enc_idx_vec = enc_idx_vec[-enc_len_att:]\n",
    "        else:\n",
    "            enc_len = _safe_to_np(x.get(\"encoder_lengths\"))\n",
    "            if enc_len is None:\n",
    "                enc_len_i = training.max_encoder_length\n",
    "            else:\n",
    "                enc_len_i = int(enc_len.reshape(-1)[n_idx_x])\n",
    "            # buat jendela encoder yang berakhir di dec_t0[n_idx_x]-1\n",
    "            enc_idx_vec = np.arange(dec_t0[n_idx_x] - enc_len_att, dec_t0[n_idx_x], dtype=int)\n",
    "    else:\n",
    "        # attention agregat (batch=1) â†’ pakai att_vec tunggal\n",
    "        aggregated = True\n",
    "        att_vec = A[0, h, :]  # (enc_len,)\n",
    "        # rekonstruksi encoder index vektor dengan panjang = enc_len_att\n",
    "        enc_idx_vec = np.arange(dec_t0[n_idx_x] - enc_len_att, dec_t0[n_idx_x], dtype=int)\n",
    "\n",
    "    # mapping ke tanggal\n",
    "    enc_dates = pd.to_datetime([t2date.get(int(t), pd.NaT) for t in enc_idx_vec])\n",
    "    close_series = np.array([t2close.get(int(t), np.nan) for t in enc_idx_vec])\n",
    "\n",
    "    plot_df = pd.DataFrame({\"Date\": enc_dates, \"attention\": att_vec, \"Close\": close_series})\n",
    "    plot_df = plot_df.dropna(subset=[\"Date\"]).sort_values(\"Date\")\n",
    "    if len(plot_df) == 0:\n",
    "        print(\"(i) Tidak ada data attention valid untuk sampel ini.\")\n",
    "        return\n",
    "    plot_df[\"attention_norm\"] = plot_df[\"attention\"] / (plot_df[\"attention\"].max() + 1e-12)\n",
    "\n",
    "    # plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 4))\n",
    "    title_extra = \" (agregat)\" if aggregated else f\" (sample idx={n_idx_x})\"\n",
    "    ax.plot(plot_df[\"Date\"], plot_df[\"attention_norm\"], label=f\"Attention (norm), H{h+1}{title_extra}\")\n",
    "    ax.set_title(f\"Temporal Attention vs Date â€” {split_name}\\n{('target_date='+str(target_ts)) if target_ts is not None else 'latest'}\")\n",
    "    ax.set_xlabel(\"Date\")\n",
    "    ax.set_ylabel(\"Attention (normalized)\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    if overlay_price:\n",
    "        ax2 = ax.twinx()\n",
    "        ax2.plot(plot_df[\"Date\"], plot_df[\"Close\"], alpha=0.35)\n",
    "        ax2.set_ylabel(\"Close (overlay)\")\n",
    "\n",
    "    ax.legend(loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Top-10 tanggal paling â€œdiperhatikanâ€\n",
    "    topk = plot_df.sort_values(\"attention\", ascending=False).head(10)\n",
    "    print(\"Top-10 encoder dates by attention:\")\n",
    "    display(topk[[\"Date\",\"attention\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57cd61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 23: contoh penggunaan =====\n",
    "\n",
    "# 1) Sampel TEST paling baru (default)\n",
    "plot_temporal_attention_by_date(eval_test_loader, split_name=\"TEST\", target_date=None, decoder_h=0, overlay_price=True)\n",
    "\n",
    "# 2) (opsional) pilih tanggal spesifik untuk titik prediksi H1,\n",
    "#    misal kamu pengin lihat prediksi untuk 2025-07-31\n",
    "# plot_temporal_attention_by_date(eval_test_loader, split_name=\"TEST\", target_date=\"2025-07-31\", decoder_h=0, overlay_price=True)\n",
    "\n",
    "# 3) (opsional) versi VAL\n",
    "# plot_temporal_attention_by_date(eval_val_loader, split_name=\"VAL\", target_date=None, decoder_h=0, overlay_price=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e090f4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(tft.state_dict(), \"tft_model.pth\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
